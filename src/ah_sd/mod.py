import torch\nimport sys\nimport asyncio\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionPipeline\nfrom nanoid import generate\nimport os\nfrom typing import Optional, Dict, Any, List # Added\n\n# Updated imports for MindRoot plugin system\nfrom lib.providers.commands import command\nfrom lib.providers.services import service\nfrom lib.hooks import hook # Assuming this is the correct path for @hook\n\npipeline = None\n\n# Global model configuration, typically set by environment variables via warmup\nif os.environ.get('AH_DEFAULT_SD_MODEL'):\n    current_model = 'models/' + os.environ.get('AH_DEFAULT_SD_MODEL')\n    local_model = True\n    use_sdxl = True # Defaulting to True if model is local, can be overridden by AH_USE_SDXL\n    from_huggingface = False\nelse:\n    current_model = 'stabilityai/stable-diffusion-xl-base-1.0' # Defaulting to a common SDXL model\n    local_model = False\n    use_sdxl = True # Defaulting to SDXL for HuggingFace\n    from_huggingface = True\n\nif os.environ.get('AH_USE_SDXL') == 'True':\n    use_sdxl = True\nelif os.environ.get('AH_USE_SDXL') == 'False':\n    use_sdxl = False\n    if not os.environ.get('AH_DEFAULT_SD_MODEL'): # If not local and SDXL is false, use a non-XL model\n        current_model = 'runwayml/stable-diffusion-v1-5'\n\ndef random_img_fname():\n    img_dir = \"imgs\"\n    if not os.path.exists(img_dir):\n        try:\n            os.makedirs(img_dir)\n        except OSError as e:\n            print(f\"Error creating directory {img_dir}: {e}\", file=sys.stderr)\n            img_dir = \".\"\n    return os.path.join(img_dir, generate() + \".png\")\n\ndef use_model(model_id: str, local: bool = True, is_sdxl: Optional[bool] = None):\n    global current_model, local_model, use_sdxl, from_huggingface, pipeline\n    current_model = model_id\n    local_model = local\n    from_huggingface = not local\n    if is_sdxl is not None:\n        use_sdxl = is_sdxl\n    pipeline = None # Force reinitialization\n\ndef get_pipeline_embeds(pipe, prompt: str, negative_prompt: str, device: str):\n    max_length = pipe.tokenizer.model_max_length\n\n    input_ids = pipe.tokenizer(prompt, return_tensors=\"pt\", truncation=False).input_ids.to(device)\n    negative_ids = pipe.tokenizer(negative_prompt, return_tensors=\"pt\", truncation=False).input_ids.to(device)\n\n    shape_max_length = max(input_ids.shape[-1], negative_ids.shape[-1])\n\n    if input_ids.shape[-1] < shape_max_length:\n        input_ids = pipe.tokenizer(prompt, return_tensors=\"pt\", truncation=False, padding=\"max_length\", max_length=shape_max_length).input_ids.to(device)\n    if negative_ids.shape[-1] < shape_max_length:\n        negative_ids = pipe.tokenizer(negative_prompt, return_tensors=\"pt\", truncation=False, padding=\"max_length\", max_length=shape_max_length).input_ids.to(device)\n\n    concat_embeds = []\n    neg_embeds = []\n    for i in range(0, shape_max_length, max_length):\n        concat_embeds.append(pipe.text_encoder(input_ids[:, i: i + max_length])[0])\n        neg_embeds.append(pipe.text_encoder(negative_ids[:, i: i + max_length])[0])\n    \n    prompt_embeds = torch.cat(concat_embeds, dim=1)\n    negative_prompt_embeds = torch.cat(neg_embeds, dim=1)\n    return prompt_embeds, negative_prompt_embeds\n\n@hook()\nasync def warmup(context: Optional[Any] = None):\n    global from_huggingface, current_model, pipeline, local_model, use_sdxl\n \n    if pipeline is not None:\n        print(f\"Pipeline for {current_model} already initialized.\")\n        return\n\n    print(f\"Warmup: Initializing pipeline. Model: {current_model}, SDXL: {use_sdxl}, Local: {local_model}, HF: {from_huggingface}\")\n\n    try:\n        if use_sdxl:\n            print(f\"Initializing StableDiffusionXLPipeline for {current_model}...\")\n            if not from_huggingface:\n                pipeline = StableDiffusionXLPipeline.from_single_file(current_model, torch_dtype=torch.float16, use_safetensors=True if current_model.endswith('.safetensors') else False)\n            else:\n                pipeline = StableDiffusionXLPipeline.from_pretrained(current_model, torch_dtype=torch.float16, use_safetensors=True)\n        else:\n            print(f\"Initializing StableDiffusionPipeline for {current_model}...\")\n            if not from_huggingface:\n                pipeline = StableDiffusionPipeline.from_single_file(current_model, torch_dtype=torch.float16, use_safetensors=True if current_model.endswith('.safetensors') else False)\n            else:\n                pipeline = StableDiffusionPipeline.from_pretrained(current_model, torch_dtype=torch.float16, use_safetensors=True)\n        \n        pipeline = pipeline.to(\"cuda\")\n        print(f\"Pipeline for {current_model} loaded to CUDA.\")\n\n        if not local_model and hasattr(pipeline, 'safety_checker') and pipeline.safety_checker is not None:\n            print(\"Disabling safety checker for HuggingFace model.\")\n            pipeline.safety_checker = lambda images, **kwargs: (images, [False]*len(images))\n    except Exception as e:\n        print(f\"Error during pipeline initialization: {e}\", file=sys.stderr)\n        pipeline = None\n\n@service()\nasync def text_to_image(prompt: str, negative_prompt: str = '', \n                        model_id: Optional[str] = None, \n                        from_huggingface_flag: Optional[bool] = None, \n                        is_sdxl_flag: Optional[bool] = None,\n                        count: int = 1, context: Optional[Any] = None, \n                        save_to: Optional[str] = None, \n                        w: int = 1024, h: int = 1024, \n                        steps: int = 20, cfg: float = 8.0) -> Optional[str]: # Updated signature\n    global pipeline, current_model, use_sdxl, local_model, from_huggingface # Ensure all relevant globals are accessible\n\n    if model_id is not None:\n        is_local_model_request = not from_huggingface_flag if from_huggingface_flag is not None else not model_id.startswith('http') # Basic inference\n        if model_id != current_model or \
           (is_sdxl_flag is not None and is_sdxl_flag != use_sdxl) or \
           (is_local_model_request != local_model):\n            print(f\"Model change requested by service. New model: {model_id}, Current: {current_model}, New SDXL: {is_sdxl_flag}, Current SDXL: {use_sdxl}\")\n            use_model(model_id, local=is_local_model_request, is_sdxl=is_sdxl_flag)\n    \n    if pipeline is None:\n        print(\"Pipeline not initialized. Calling warmup...\")\n        await warmup(context=context)\n        if pipeline is None:\n            print(\"Pipeline initialization failed. Cannot generate image.\", file=sys.stderr)\n            return None\n\n    images_fnames = []\n    for n in range(count):\n        actual_w = w if w != 1024 else (1024 if use_sdxl else 512)\n        actual_h = h if h != 1024 else (1024 if use_sdxl else 512)\n        if w != 1024 : actual_w = w # If user specified w, use it\n        if h != 1024 : actual_h = h # If user specified h, use it\n\n        print(f\"Generating image {n+1}/{count} with prompt: '{prompt[:50]}...' model: {current_model} SDXL: {use_sdxl} W: {actual_w} H: {actual_h}\")\n        \n        prompt_embeds, negative_prompt_embeds = get_pipeline_embeds(pipeline, prompt, negative_prompt, \"cuda\")\n\n        image_obj = pipeline(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_prompt_embeds,\n                             width=actual_w, height=actual_h,\n                             num_inference_steps=steps, guidance_scale=cfg).images[0]\n        \n        fname_to_save = save_to if save_to and count == 1 else random_img_fname()\n        \n        try:\n            image_obj.save(fname_to_save)\n            print(f\"Image saved to {fname_to_save}\")\n            images_fnames.append(fname_to_save)\n        except Exception as e:\n            print(f\"Error saving image to {fname_to_save}: {e}\", file=sys.stderr)\n\n    return images_fnames[0] if count == 1 and images_fnames else None # Simplified return for single image\n\n\n@command()\nasync def image(prompt: str, negative_prompt: str = \"\", \n                steps: int = 20, cfg: float = 8.0, \n                w: int = 1024, h: int = 1024, \n                context: Optional[Any] = None) -> Optional[Dict[str, str]]: # Updated signature\n    \"\"\"image: Generate an image from a prompt using the currently configured model.\n\n    Args:\n        prompt (str): The text prompt to generate the image from.\n        negative_prompt (str, optional): Text prompt to guide what not to include. Defaults to ''.\n        steps (int, optional): Number of inference steps. Defaults to 20.\n        cfg (float, optional): Classifier-Free Guidance scale. Defaults to 8.0.\n        w (int, optional): Width of the generated image. Defaults to 1024 (or 512 for non-SDXL default model).\n        h (int, optional): Height of the generated image. Defaults to 1024 (or 512 for non-SDXL default model).\n        context (Optional[Any]): The execution context.\n\n    Example:\n    { \"image\": { \"prompt\": \"A cute tabby cat in the forest\" } }\n\n    Example with more parameters:\n    { \"image\": { \n        \"prompt\": \"A futuristic cityscape at sunset\", \n        \"negative_prompt\": \"ugly, blurry, cars\", \n        \"w\": 768, \n        \"h\": 768,\n        \"steps\": 25,\n        \"cfg\": 7.0\n      }\n    }\n    \"\"\"\n    fname = await text_to_image(prompt=prompt, negative_prompt=negative_prompt,\n                                model_id=None, from_huggingface_flag=None, is_sdxl_flag=None, \n                                w=w, h=h, steps=steps, cfg=cfg, context=context)\n    \n    if fname and isinstance(fname, str):\n        print(f\"Image command output to file: {fname}\")\n        if hasattr(context, 'insert_image'):\n            img_filename = os.path.basename(fname)\n            rel_url = f\"/imgs/{img_filename}\"\n            await context.insert_image(rel_url)\n            return { \"status\": \"generated\", \"file_path\": rel_url, \"message\": f\"Image generated at {rel_url} and inserted.\"}\n        else:\n            print(\"Context does not have insert_image method.\", file=sys.stderr)\n            return { \"status\": \"generated_no_insert\", \"file_path\": fname, \"message\": \"Image generated but could not be inserted into chat.\"}\n    else:\n        print(\"Image generation failed or returned no filename.\", file=sys.stderr)\n        if hasattr(context, 'send_message'):\n            await context.send_message(\"Sorry, I couldn't generate the image.\")\n        return { \"status\": \"error\", \"message\": \"Image generation failed.\" }\n\n